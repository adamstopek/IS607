#### Stacey Schwarcz ####
#Programming Entropy and Information Gain

#For this project, you will be creating a collection of functions that analyze categorical data using the concepts of 
#entropy and information gain.
#The attached .CSV file will provide you with some data to test your function. The functions described in the attached 
#.PDF should be coded, tested, and placed in a single R script. #Place your R script in a public repository on github 
#and submit a link to the script here. 
#Entropy Project is due end of day on Tuesday September 16th.


#### 1.#### 
#   Create a function entropy() that takes a vector d as input and returns a single numeric value that is the 
#   entropy E(d) of the vector:E(d)=- SUM(i=1 to k) of (p(sub i)*log(base 2)*pi)
#   Note that your function should accept as input a vector with data type factor, character, or numeric, 
#   even though the data will be treated as categorical data. Call this function

entropy<-function(d)
{
  n<-length(d)
  t<-table(d)
  p<-t/n
  f<-p*log2(p)
  f[is.na(f)] <-0
  E<--sum(f)
  E
  #list(n,t,p,f,E) #test interim outputs
}
#output for test functions checks out

#### 2. ####
#   Create a function infogain() that takes two vectors - the target d and the attribute a with which to partition 
#   the data - and returns the information gain I(d,a) for the attribute:  
#   I(d,a) = E(d) - SUM(j=1 to m) of (n(sub j)/n)*E(d(sub j)
#   where n is the length of the target d, n(sub j) is the size of the partition of d according to the jth value 
#   of the attribute a, and E(dj) is the entropy of the partition of d according to the jth value of the attribute a.

infogain<-function(d,a)
{
  n<-length(d)
  df<-data.frame(d,a)
  asplit<-split(df,df$a)
  dsplit<-split(df,list(df$a,df$d))
  #ha<-lapply(asplit,head) #view dsplit
  #hd<-lapply(hsplit,head) #view asplit
  nj<-sapply(asplit,nrow)
  ni<-sapply(dsplit,nrow)
  p1<-ni/n
  p2<-ni/nj
  fj<-p1*log2(p2)
  fj[is.na(fj)] <-0
  Edj<--sum(fj)
  E.d<-entropy(d)
  L<-E.d-Edj
  #list("n"=n,"nj"=nj,"ni"=ni,"p1"=p1,"p2"=p2,"fj"=fj,"Edj"=Edj,"E.d"=E.d,"L"=L)  #to test interim outputs
  L
}
#output for test functions check out

#### 3.####
#   Create a function decide() that takes a data frame - the target d and the collection of candidate attributes 
#   a(sub i) with which to partition the data - and the number of the column that is the target and returns a list 
#   containing two items: the identity (by column number) of the attribute that maximizes the information gain and 
#   a vector of the information gains for each of the candidate attributes.

decide<-function(DF,c)
{
  gains<-numeric()
  for (i in 1:(c-1)){
    a<-DF[[i]]
    d<-DF[[c]]
    e<-infogain(d,a)
    gains<-c(gains,e)
  }
  max<-match(max(gains),gains)
  list(max=max,gains=gains)
}
#output for test functions check out.

#Store your three functions in an R script called entropyfunctions.R. Your submission here should consist 
#of a single link to the file in your github repository.#A sample dataset (entropy-sample.csv) has been 
#provided. The functions you create should be able to produce the following results when run against this dataset: 

dataset<-read.csv("c:/Users/trans_000/Documents/GitHub/IS607/IS607/entropy-test-file.csv") #desktop
#dataset<-read.csv("c:/IS 607/entropy-test-file.csv") #alternate computer
entropy(dataset$answer) 
#[1] 0.9832692 
infogain(dataset$answer,dataset$attr1) 
#[1] 2.411565e-05 
infogain(dataset$answer,dataset$attr2) 
#[1] 0.2599038
infogain(dataset$answer,dataset$attr3) 
#[1] 0.002432707 
decide(dataset,4) 
#$max 
#[1] 2 
#$gains 
#attr1 attr2 attr3 
#2.411565e-05 2.599038e-01 2.432707e-03
